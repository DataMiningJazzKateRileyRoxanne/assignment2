{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENEL-645 Assignment 2\n",
    "\n",
    "\n",
    "Team Members:\n",
    "Jaskirat Singh\n",
    "Kate Reimann\n",
    "Riley Koppang\n",
    "Roxanne Mai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Intro and purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Hugging Face transformers for text\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// Data pre-processing (split already done)\n",
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset that returns:\n",
    "      - an image (loaded & transformed),\n",
    "      - tokenized text input_ids, attention_mask,\n",
    "      - and a label.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 image_paths, \n",
    "                 texts, \n",
    "                 labels, \n",
    "                 tokenizer, \n",
    "                 image_transform=None, \n",
    "                 max_text_len=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (List[str]): Paths to image files.\n",
    "            texts (List[str]): Corresponding text for each image.\n",
    "            labels (List[int]): Integer labels for classification.\n",
    "            tokenizer: DistilBertTokenizer (or similar).\n",
    "            image_transform: torchvision transforms for image.\n",
    "            max_text_len (int): Maximum tokens for DistilBERT.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ---- Get image ----\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "\n",
    "        # ---- Get text ----\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].squeeze()       # shape: [max_text_len]\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        # ---- Get label ----\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return {\n",
    "            'image': image, \n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask': attention_mask, \n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforms and tokenizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example image transform (you can tweak as needed)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # Normalization for ImageNet:\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4, projection_dim=128):\n",
    "        \"\"\"\n",
    "        num_classes: number of output classes for final classification.\n",
    "        projection_dim: dimension for x after the Dense projection for both text and image.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "\n",
    "        # ----- Image Feature Extractor (ResNet) -----\n",
    "        # We'll use a pretrained ResNet18. \n",
    "        # You can choose weights='IMAGENET1K_V1' or similar in newer torchvision versions.\n",
    "        self.image_model = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove the final classification layer (fc) so we get a 512-d or 1000-d feature.\n",
    "        # By default, ResNet18's fc out_features=1000. We'll keep that for now.\n",
    "        # If you want the 512-d embedding, you can do:\n",
    "        #  self.image_model.fc = nn.Identity()\n",
    "        #  then you'd have to know it's 512 dims. \n",
    "        # Here, we'll just keep the 1000-d final.\n",
    "        \n",
    "        # ----- Text Feature Extractor (DistilBERT) -----\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # DistilBERT hidden_size is typically 768. We can confirm as:\n",
    "        text_hidden_size = self.text_model.config.hidden_size\n",
    "        \n",
    "        # ----- Projection Layers (Dense -> x) -----\n",
    "        # For images: input size is 1000 because resnet18(fc) outputs 1000\n",
    "        # For text: input size is text_hidden_size (768 for distilbert-base-uncased)\n",
    "        \n",
    "        self.image_proj = nn.Linear(1000, projection_dim)\n",
    "        self.text_proj  = nn.Linear(text_hidden_size, projection_dim)\n",
    "        \n",
    "        # ----- Final Classification -----\n",
    "        # We'll combine the two projected vectors by concatenation -> dimension is 2 * projection_dim\n",
    "        self.classifier = nn.Linear(2 * projection_dim, num_classes)\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        images: Tensor [batch, 3, 224, 224]\n",
    "        input_ids: Tensor [batch, max_len]\n",
    "        attention_mask: Tensor [batch, max_len]\n",
    "        \"\"\"\n",
    "        # ----- IMAGE FORWARD -----\n",
    "        # Pass images through ResNet. \n",
    "        # By default, ResNet includes its final fc layer, returning [batch, 1000]\n",
    "        # If you replaced that fc with an Identity() layer, you'd get [batch, 512].\n",
    "        f_image = self.image_model(images)  # shape: [batch, 1000]\n",
    "        \n",
    "        # Dense projection to dimension x\n",
    "        x_image = self.image_proj(f_image)  # shape: [batch, projection_dim]\n",
    "        \n",
    "        # Normalize \n",
    "        # This step ensures the range of values is consistent for both modalities\n",
    "        xnorm_image = F.normalize(x_image, p=2, dim=1)\n",
    "        \n",
    "        # ----- TEXT FORWARD -----\n",
    "        # DistilBERT returns a tuple: (last_hidden_state, ...)\n",
    "        # last_hidden_state shape: [batch, seq_len, hidden_size]\n",
    "        # We often take the first token ([CLS]) or the pooled output\n",
    "        # DistilBERT doesn't have a [CLS] token pooler, so we often take [0, 0] or average pool\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = text_outputs[0]   # shape: [batch, seq_len, hidden_size]\n",
    "        \n",
    "        # We'll just take the first token's embedding (like BERT's [CLS])\n",
    "        # Alternatively, you could pool or average\n",
    "        cls_text = last_hidden_state[:, 0]    # shape: [batch, hidden_size]\n",
    "        \n",
    "        # Dense projection to dimension x\n",
    "        x_text = self.text_proj(cls_text)     # shape: [batch, projection_dim]\n",
    "        \n",
    "        # Normalize\n",
    "        xnorm_text = F.normalize(x_text, p=2, dim=1)\n",
    "        \n",
    "        # ----- COMBINE & CLASSIFY -----\n",
    "        # Concatenate the normalized features\n",
    "        combined = torch.cat([xnorm_image, xnorm_text], dim=1)  # shape: [batch, 2*projection_dim]\n",
    "        \n",
    "        # Final linear for classification\n",
    "        logits = self.classifier(combined)    # shape: [batch, num_classes]\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
